{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAM-MoE Quickstart Guide\n",
    "\n",
    "This notebook will guide you through compressing a Mixture-of-Experts (MoE) model using the REAM-MoE library.\n",
    "\n",
    "## What is REAM-MoE?\n",
    "\n",
    "REAM-MoE compresses large language models with MoE architectures by:\n",
    "- **Expert Pruning**: Removing low-importance experts\n",
    "- **Expert Merging**: Combining similar experts using permutation-aware averaging\n",
    "\n",
    "This reduces model size and inference cost while preserving most of the model's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library (if needed)\n",
    "# !pip install -e .\n",
    "\n",
    "# Or install from requirements\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries and configure logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Import REAM-MoE components\n",
    "from ream_moe import (\n",
    "    observe_model,\n",
    "    prune_model,\n",
    "    merge_model,\n",
    "    PruningConfig,\n",
    "    MergeConfig,\n",
    "    verify_model_config,\n",
    "    print_verification_result,\n",
    "    setup_logging,\n",
    ")\n",
    "from ream_moe.calibration import build_calibration_batches, list_available_datasets\n",
    "\n",
    "# Optional: suppress INFO logs to reduce noise\n",
    "setup_logging(level=\"WARNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "Load your MoE model and tokenizer. This example uses Qwen3-14B-MoE, but you can use any supported model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-14B-MoE\"  # Change to your model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded model: {model.__class__.__name__}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Model Configuration\n",
    "\n",
    "Check if your model is properly supported by REAM-MoE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model configuration\n",
    "result = verify_model_config(MODEL_NAME, model)\n",
    "print_verification_result(result)\n",
    "\n",
    "if not result[\"valid\"]:\n",
    "    print(\"\\nWarning: Model verification failed. You may need to add configuration for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Calibration Data\n",
    "\n",
    "Build calibration batches to collect activation statistics. You can use built-in datasets or your own texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available built-in datasets\n",
    "print(\"Available calibration datasets:\")\n",
    "for ds in list_available_datasets():\n",
    "    print(f\"  - {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build calibration batches\n",
    "# Options: \"c4\", \"code\", \"math\", \"writing\", \"hardcoded\", \"combined\"\n",
    "DATASET = \"hardcoded\"  # Recommended - uses diverse hardcoded prompts\n",
    "SAMPLES = 2000  # Number of samples for calibration\n",
    "MAX_SEQ_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "batches = list(build_calibration_batches(\n",
    "    tokenizer,\n",
    "    DATASET,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    samples=SAMPLES,\n",
    "))\n",
    "\n",
    "print(f\"Created {len(batches)} calibration batches\")\n",
    "print(f\"Total samples: {len(batches) * BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect Activation Statistics\n",
    "\n",
    "Run the model on calibration data to collect expert activation statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect observer data\n",
    "observer_data = {}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(batches):\n",
    "        if i >= 100:  # Limit batches for quick testing\n",
    "            break\n",
    "        \n",
    "        # Get batch data\n",
    "        input_ids = batch.input_ids.to(DEVICE)\n",
    "        attention_mask = batch.attention_mask.to(DEVICE)\n",
    "        \n",
    "        # Collect statistics\n",
    "        batch_data = observe_model(\n",
    "            model,\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Merge batch data\n",
    "        for layer_idx, data in batch_data.items():\n",
    "            if layer_idx not in observer_data:\n",
    "                observer_data[layer_idx] = data\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(batches)} batches\")\n",
    "\n",
    "print(f\"\\nCollected statistics for {len(observer_data)} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compress the Model\n",
    "\n",
    "### Option A: Expert Pruning\n",
    "\n",
    "Remove low-importance experts to reduce model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune 25% of experts (keep 75%)\n",
    "prune_config = PruningConfig(\n",
    "    compression_ratio=0.25,  # Remove 25% of experts\n",
    "    prune_method=\"saliency_scores\",  # Use REAP saliency\n",
    "    preserve_super_experts=False,  # Set to True to preserve high-activation experts\n",
    ")\n",
    "\n",
    "retained_counts = prune_model(model, observer_data, prune_config)\n",
    "\n",
    "avg_experts = sum(retained_counts.values()) / len(retained_counts)\n",
    "print(f\"Average experts per layer after pruning: {avg_experts:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Expert Merging (Alternative)\n",
    "\n",
    "Instead of pruning, you can merge similar experts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge experts to keep 75% (25% compression)\n",
    "# Uncomment to use merging instead of pruning\n",
    "# merge_config = MergeConfig(\n",
    "#     target_ratio=0.75,  # Keep 75% of experts\n",
    "#     group_size=16,  # Max experts per group\n",
    "# )\n",
    "# retained_counts = merge_model(model, observer_data, merge_config)\n",
    "# \n",
    "# avg_experts = sum(retained_counts.values()) / len(retained_counts)\n",
    "# print(f\"Average experts per layer after merging: {avg_experts:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Compressed Model\n",
    "\n",
    "Save the compressed model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./compressed_model\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Compression info:\")\n",
    "print(f\"  - Original experts: Check model.config\")\n",
    "print(f\"  - Compressed experts: {avg_experts:.1f} per layer\")\n",
    "print(f\"  - Compression ratio: ~{prune_config.compression_ratio:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Compressed Model\n",
    "\n",
    "Load and test the compressed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compressed model\n",
    "compressed_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = compressed_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Options\n",
    "\n",
    "### Using Custom Calibration Data\n",
    "\n",
    "You can use your own texts for calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom texts\n",
    "my_texts = [\n",
    "    \"Your custom text here...\",\n",
    "    \"More text samples...\",\n",
    "    \"Add as many as you want\",\n",
    "]\n",
    "\n",
    "custom_batches = list(build_calibration_batches(\n",
    "    tokenizer,\n",
    "    my_texts,\n",
    "    max_seq_len=512,\n",
    "    batch_size=4,\n",
    "))\n",
    "\n",
    "print(f\"Created {len(custom_batches)} custom calibration batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Supported Models\n",
    "\n",
    "Check which model families are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ream_moe import list_supported_models\n",
    "\n",
    "print(\"Supported model families:\")\n",
    "for model_class in list_supported_models():\n",
    "    print(f\"  - {model_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Best Practices\n",
    "\n",
    "1. **Calibration Data Quality**: Use diverse, representative text for calibration. The \"hardcoded\" dataset covers multiple domains.\n",
    "\n",
    "2. **Compression Ratio**: Start with 0.25 (25% compression) and adjust based on quality vs. size tradeoff.\n",
    "\n",
    "3. **Preserve Super Experts**: Enable `preserve_super_experts=True` if you notice important capabilities degrading.\n",
    "\n",
    "4. **Verification**: Always verify your model configuration before compressing.\n",
    "\n",
    "5. **Testing**: Thoroughly test your compressed model on your specific use cases.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Model not supported**: Use `verify_model_config()` to check. The library may auto-detect unknown models.\n",
    "\n",
    "- **Out of memory**: Reduce `batch_size`, `max_seq_len`, or `samples`. Use `store_on_cpu=True` in ObserverConfig.\n",
    "\n",
    "- **Quality degradation**: Try lower compression ratio or enable `preserve_super_experts`.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check the [README](../README.md) for CLI usage\n",
    "- Explore different calibration datasets for your use case\n",
    "- Experiment with both pruning and merging methods\n",
    "- Contribute configurations for unsupported models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
